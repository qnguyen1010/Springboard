{"cells":[{"metadata":{},"cell_type":"markdown","source":"#### My own functions"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_hist(df, col, label):\n    plt.hist(df[col], label = label)\n    plt.legend()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1. Data Preparation"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# General imports\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nimport os, sys, gc, warnings, random, datetime\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split, KFold\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm\n\nimport math\n\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"########################### Helpers\n#################################################################################\n## -------------------\n## Seeder\n# :seed to make all processes deterministic     # type: int\ndef seed_everything(seed=0):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n## ------------------- \n\n## -------------------\n## Memory Reducer\n# :df pandas dataframe to reduce size             # type: pd.DataFrame()\n# :verbose                                        # type: bool\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df\n## -------------------","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"########################### Vars\n#################################################################################\nSEED = 42\nseed_everything(SEED)\nLOCAL_TEST = False\nTARGET = 'isFraud'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"########################### DATA LOAD\n#################################################################################\nprint('Load Data')\ntrain_df = pd.read_csv('../input/ieee-fraud-detection/train_transaction.csv')\ntest_df = pd.read_csv('../input/ieee-fraud-detection/test_transaction.csv')\ntest_df['isFraud'] = 0\n\ntrain_identity = pd.read_csv('../input/ieee-fraud-detection/train_identity.csv')\ntest_identity = pd.read_csv('../input/ieee-fraud-detection/test_identity.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"########################### Base check\n#################################################################################\n\nif LOCAL_TEST:\n    for df2 in [train_df, test_df, train_identity, test_identity]:\n        df = reduce_mem_usage(df2)\n\n        for col in list(df):\n            if not df[col].equals(df2[col]):\n                print('Bad transformation', col)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"########################### Base Minification\n#################################################################################\n\ntrain_df = reduce_mem_usage(train_df)\ntest_df  = reduce_mem_usage(test_df)\n\ntrain_identity = reduce_mem_usage(train_identity)\ntest_identity  = reduce_mem_usage(test_identity)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"########################### Columns\n#################################################################################\n## Main Data\n# 'TransactionID',\n# 'isFraud',\n# 'TransactionDT',\n# 'TransactionAmt',\n# 'ProductCD',\n# 'card1' - 'card6',\n# 'addr1' - 'addr2',\n# 'dist1' - 'dist2',\n# 'P_emaildomain' - 'R_emaildomain',\n# 'C1' - 'C14'\n# 'D1' - 'D15'\n# 'M1' - 'M9'\n# 'V1' - 'V339'\n\n## Identity Data\n# 'TransactionID'\n# 'id_01' - 'id_38'\n# 'DeviceType',\n# 'DeviceInfo'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2. Data Preprocessing"},{"metadata":{},"cell_type":"markdown","source":"**Dataset Columns**\n\n**Transaction Data**\n\n'TransactionID'\n\n'isFraud'\n\n'TransactionDT'\n\n'TransactionAmt'\n\n'ProductCD' **Categorical Features**\n\n'card1' - 'card6' **Categorical Features**\n\n'addr1' - 'addr2' **Categorical Features**\n\n'dist1' - 'dist2'\n\n'P_emaildomain' **Categorical Features**\n\n'R_emaildomain' **Categorical Features**\n\n'C1' - 'C14'\n\n'D1' - 'D15'\n\n'M1' - 'M9' **Categorical Features**\n\n'V1' - 'V339'\n\n\n\n\n**Identity Data**\n\n'TransactionID'\n\n'id_01' - 'id_38' **Categorical Features id_12 - id_38**\n\n'DeviceType' **Categorical Features**\n\n'DeviceInfo' **Categorical Features**"},{"metadata":{},"cell_type":"markdown","source":"#### 2.1 Product CD"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,6))\n\ntotal = len(train_df)\n\nplt.subplot(121)\ng = sns.countplot(x = 'ProductCD', data = train_df)\ng.set_title('ProductCD Distribution', fontsize = 15)\ng.set_xlabel(\"Product Code\", fontsize=15)\ng.set_ylabel(\"Count\", fontsize=15)\nfor p in g.patches:\n    height = p.get_height()\n    g.text(p.get_x()+p.get_width()/2.,\n            height + 3,\n            '{:1.2f}%'.format(height/total*100),\n            ha=\"center\", fontsize=14) \n\nplt.subplot(122)\ng1 = sns.countplot(x='ProductCD', hue='isFraud', data=train_df)\ng1.set_title('ProductCD by Fraud', fontsize = 15)\ng1.set_xlabel(\"Product Code\", fontsize=15)\ng1.set_ylabel(\"Count\", fontsize=15)\nplt.legend(title='Fraud', loc='best', labels=['No', 'Yes'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[train_df['isFraud'] == 1]['ProductCD'].value_counts(normalize = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (12,12))\ntemp = train_df.groupby('ProductCD')['isFraud'].value_counts(normalize = True).unstack()\na = temp.plot.bar(stacked = True)\na.set_title('Rate of Fraud by Product Code', fontsize = 15)\nplt.xticks(rotation = 'horizontal')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These results susgest that with product code, we need to do 2 things:\n* Encode categorical levels using Frequency Encoding\n* Add target mean by categorical level. It is found here that product C has highest fraud rate, compared to others"},{"metadata":{"trusted":true},"cell_type":"code","source":"# ProductCD Frequency Encoding\ncol = 'ProductCD'\ntemp_df = pd.concat([train_df[[col]], test_df[[col]]])   # I don't want to use test data\n#temp_df = train_df[[col]]\ncol_encoded = temp_df[col].value_counts().to_dict()\ntrain_df[col] = train_df[col].map(col_encoded)\ntest_df[col]  = test_df[col].map(col_encoded)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## ProductCD Target mean\nfor col in ['ProductCD']:\n    temp_dict = train_df.groupby([col])[TARGET].agg(['mean']).reset_index().rename(\n                                                        columns={'mean': col+'_target_mean'})\n    temp_dict.index = temp_dict[col].values\n    temp_dict = temp_dict[col+'_target_mean'].to_dict()\n\n    train_df[col+'_target_mean'] = train_df[col].map(temp_dict)\n    test_df[col+'_target_mean']  = test_df[col].map(temp_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[['ProductCD', 'ProductCD_target_mean']].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.2 Card1 - Card6"},{"metadata":{},"cell_type":"markdown","source":"#### a. Card 1, 2, 3, 5\n\nThe card 1,2,3, and 5 was represented as numerical values, temping us to plot the histogram. However, we need to remember that card columns were classified as categorical variables. Meaning it's likely that these numerical variables were coded for categorical variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.describe().loc[:,'card1':'card5']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.loc[:,'card1':'card5'].nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Card 1 contains 13553 unique values, suggesting card 1 may have been ID of the card. Card 2,3 and 5 have less unique values, so perhaps they could be expiration date, or combinations that generate card identity? Since we don't know how these information was scrammbled, we might pickup patterns generated by encryption algorithm instead of data. We can calculate target mean grouped by 'card1' - 'card5' (except 'card4'"},{"metadata":{"trusted":true},"cell_type":"code","source":"## ProductCD Target mean\nfor col in ['card1', 'card2', 'card3', 'card5']:\n    temp_dict = train_df.groupby([col])[TARGET].agg(['mean']).reset_index().rename(\n                                                        columns={'mean': col+'_target_mean'})\n    temp_dict.index = temp_dict[col].values\n    temp_dict = temp_dict[col+'_target_mean'].to_dict()\n\n    train_df[col+'_target_mean'] = train_df[col].map(temp_dict)\n    test_df[col+'_target_mean']  = test_df[col].map(temp_dict)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"b. Card4"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,6))\n\ntotal = len(train_df)\n\nplt.subplot(121)\ng = sns.countplot(x = 'card4', data = train_df)\ng.set_title('Card Network Distribution', fontsize = 15)\ng.set_xlabel(\"Card Issuers\", fontsize=15)\ng.set_ylabel(\"Count\", fontsize=15)\nfor p in g.patches:\n    height = p.get_height()\n    g.text(p.get_x()+p.get_width()/2.,\n            height + 3,\n            '{:1.2f}%'.format(height/total*100),\n            ha=\"center\", fontsize=14) \n\nplt.subplot(122)\ng1 = sns.countplot(x='card4', hue='isFraud', data=train_df)\ng1.set_title('Card Network by Fraud', fontsize = 15)\ng1.set_xlabel(\"Card Issuers\", fontsize=15)\ng1.set_ylabel(\"Count\", fontsize=15)\nplt.legend(title='Fraud', loc='best', labels=['No', 'Yes'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# visualization of table\nplt.figure(figsize=(12,12))\nb = train_df.groupby('card4')['isFraud'].value_counts(normalize = True).unstack().plot.bar(stacked = True)\nb.set_title('Rate of Fraud by Card Network', fontsize = 15)\nplt.xticks(rotation='horizontal')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Card4 Frequency Encoding\ncol = 'card4'\ntemp_df = pd.concat([train_df[[col]], test_df[[col]]])   # I don't want to use test data\n#temp_df = train_df[[col]]\ncol_encoded = temp_df[col].value_counts().to_dict()\ntrain_df[col] = train_df[col].map(col_encoded)\ntest_df[col]  = test_df[col].map(col_encoded)\n\n## Card4 Target mean\nfor col in ['card4']:\n    temp_dict = train_df.groupby([col])[TARGET].agg(['mean']).reset_index().rename(\n                                                        columns={'mean': col+'_target_mean'})\n    temp_dict.index = temp_dict[col].values\n    temp_dict = temp_dict[col+'_target_mean'].to_dict()\n\n    train_df[col+'_target_mean'] = train_df[col].map(temp_dict)\n    test_df[col+'_target_mean']  = test_df[col].map(temp_dict)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"c. Card6"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,6))\n\ntotal = len(train_df)\n\nplt.subplot(121)\ng = sns.countplot(x = 'card6', data = train_df)\ng.set_title('Card Type Distribution', fontsize = 15)\ng.set_xlabel(\"Card Type\", fontsize=15)\ng.set_ylabel(\"Count\", fontsize=15)\nfor p in g.patches:\n    height = p.get_height()\n    g.text(p.get_x()+p.get_width()/2.,\n            height + 3,\n            '{:1.2f}%'.format(height/total*100),\n            ha=\"center\", fontsize=14) \n\nplt.subplot(122)\ng1 = sns.countplot(x='card6', hue='isFraud', data=train_df)\ng1.set_title('Card Type by Fraud', fontsize = 15)\ng1.set_xlabel(\"Card Type\", fontsize=15)\ng1.set_ylabel(\"Count\", fontsize=15)\nplt.legend(title='Fraud', loc='best', labels=['No', 'Yes'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# visualization of table\nplt.figure(figsize=(12,12))\nc = train_df.groupby('card6')['isFraud'].value_counts(normalize = True).unstack().plot.bar(stacked = True)\nc.set_title('Rate of Fraud by Card Type', fontsize = 15)\nplt.xticks(rotation='horizontal')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Credit card has much higher fraud rate as compared with other types of card"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Card6 Frequency Encoding\ncol = 'card6'\ntemp_df = pd.concat([train_df[[col]], test_df[[col]]])   # I don't want to use test data\n#temp_df = train_df[[col]]\ncol_encoded = temp_df[col].value_counts().to_dict()\ntrain_df[col] = train_df[col].map(col_encoded)\ntest_df[col]  = test_df[col].map(col_encoded)\n\n## Card6 Target mean\nfor col in ['card6']:\n    temp_dict = train_df.groupby([col])[TARGET].agg(['mean']).reset_index().rename(\n                                                        columns={'mean': col+'_target_mean'})\n    temp_dict.index = temp_dict[col].values\n    temp_dict = temp_dict[col+'_target_mean'].to_dict()\n\n    train_df[col+'_target_mean'] = train_df[col].map(temp_dict)\n    test_df[col+'_target_mean']  = test_df[col].map(temp_dict)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.3 Addr1 - Addr2"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_hist(train_df, 'addr1', 'addr1')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_hist(train_df, 'addr2', 'addr2')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(x='addr1', y ='isFraud', data = train_df)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# addr1 Frequency Encoding\ncol = 'addr1'\ntemp_df = pd.concat([train_df[[col]], test_df[[col]]])   # I don't want to use test data\n#temp_df = train_df[[col]]\ncol_encoded = temp_df[col].value_counts().to_dict()\ntrain_df[col] = train_df[col].map(col_encoded)\ntest_df[col]  = test_df[col].map(col_encoded)\n\n## Addr1 - Addr2 Target mean\nfor col in ['addr1', 'addr2']:\n    temp_dict = train_df.groupby([col])[TARGET].agg(['mean']).reset_index().rename(\n                                                        columns={'mean': col+'_target_mean'})\n    temp_dict.index = temp_dict[col].values\n    temp_dict = temp_dict[col+'_target_mean'].to_dict()\n\n    train_df[col+'_target_mean'] = train_df[col].map(temp_dict)\n    test_df[col+'_target_mean']  = test_df[col].map(temp_dict)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.4 Email domain"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[\"P_parent_emaildomain\"] = train_df[\"P_emaildomain\"].str.split('.', expand = True)[[0]]\ntrain_df[\"R_parent_emaildomain\"] = train_df[\"R_emaildomain\"].str.split('.', expand = True)[[0]]\n\ntest_df[\"P_parent_emaildomain\"] = test_df[\"P_emaildomain\"].str.split('.', expand = True)[[0]]\ntest_df[\"R_parent_emaildomain\"] = test_df[\"R_emaildomain\"].str.split('.', expand = True)[[0]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# P-emaildomain & R-emaildomain Frequency Encoding\nfor col in ['P_parent_emaildomain', 'R_parent_emaildomain']:\n    temp_df = pd.concat([train_df[[col]], test_df[[col]]])   # I don't want to use test data\n    #temp_df = train_df[[col]]\n    col_encoded = temp_df[col].value_counts().to_dict()\n    train_df[col] = train_df[col].map(col_encoded)\n    test_df[col]  = test_df[col].map(col_encoded)\n\n## P-emaildomain & R-emaildomain Target mean\nfor col in ['P_parent_emaildomain', 'R_parent_emaildomain']:\n    temp_dict = train_df.groupby([col])[TARGET].agg(['mean']).reset_index().rename(\n                                                        columns={'mean': col+'_target_mean'})\n    temp_dict.index = temp_dict[col].values\n    temp_dict = temp_dict[col+'_target_mean'].to_dict()\n\n    train_df[col+'_target_mean'] = train_df[col].map(temp_dict)\n    test_df[col+'_target_mean']  = test_df[col].map(temp_dict)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.5 'M1' - 'M9'"},{"metadata":{"trusted":true},"cell_type":"code","source":"#temp = train_df[train_df['isFraud']==True].groupby('M4')['isFraud'].value_counts(normalize=True)\ntemp = train_df.groupby('M4')['isFraud'].value_counts(normalize = True)\ntemp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['M4'].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## 'M4' Target mean\nfor col in ['M4']:\n    temp_dict = train_df.groupby([col])[TARGET].agg(['mean']).reset_index().rename(\n                                                        columns={'mean': col+'_target_mean'})\n    temp_dict.index = temp_dict[col].values\n    temp_dict = temp_dict[col+'_target_mean'].to_dict()\n\n    train_df[col+'_target_mean'] = train_df[col].map(temp_dict)\n    test_df[col+'_target_mean']  = test_df[col].map(temp_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in ['M1','M2','M3','M5','M6','M7','M8','M9']:\n    train_df[col] = train_df[col].map({'T':1, 'F':0})\n    test_df[col]  = test_df[col].map({'T':1, 'F':0})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's combine the data and work with the whole dataset\n\n#train = pd.merge(train_df, train_identity, on='TransactionID', how='left')\n#test = pd.merge(test_df, test_identity, on='TransactionID', how='left')\n#del train_identity, train_df, test_identity, test_df\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Reduce Memory Usage\n\n#train = reduce_mem_usage(train)\n#test = reduce_mem_usage(test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.6 'id_12' - 'id_38'"},{"metadata":{"trusted":true},"cell_type":"code","source":"#train['id_23'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate Target Mean grouped by 'id_23'\n## 'id_23' Target mean\n#for col in ['id_23']:\n#    temp_dict = train.groupby([col])[TARGET].agg(['mean']).reset_index().rename(\n#                                                        columns={'mean': col+'_target_mean'})\n#    temp_dict.index = temp_dict[col].values\n#    temp_dict = temp_dict[col+'_target_mean'].to_dict()\n\n#    train[col] = train[col].map(temp_dict)\n#    test[col]  = test[col].map(temp_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"########################### Identity columns\n#################################################################################\n\ndef minify_identity_df(df):\n\n    df['id_12'] = df['id_12'].map({'Found':1, 'NotFound':0})\n    df['id_15'] = df['id_15'].map({'New':2, 'Found':1, 'Unknown':0})\n    df['id_16'] = df['id_16'].map({'Found':1, 'NotFound':0})\n\n    df['id_23'] = df['id_23'].map({'TRANSPARENT':4, 'IP_PROXY':3, 'IP_PROXY:ANONYMOUS':2, 'IP_PROXY:HIDDEN':1})\n\n    df['id_27'] = df['id_27'].map({'Found':1, 'NotFound':0})\n    df['id_28'] = df['id_28'].map({'New':2, 'Found':1})\n\n    df['id_29'] = df['id_29'].map({'Found':1, 'NotFound':0})\n\n    df['id_35'] = df['id_35'].map({'T':1, 'F':0})\n    df['id_36'] = df['id_36'].map({'T':1, 'F':0})\n    df['id_37'] = df['id_37'].map({'T':1, 'F':0})\n    df['id_38'] = df['id_38'].map({'T':1, 'F':0})\n\n    df['id_34'] = df['id_34'].fillna(':0')\n    df['id_34'] = df['id_34'].apply(lambda x: x.split(':')[1]).astype(np.int8)\n    df['id_34'] = np.where(df['id_34']==0, np.nan, df['id_34'])\n    \n    df['id_33'] = df['id_33'].fillna('0x0')\n    df['id_33_0'] = df['id_33'].apply(lambda x: x.split('x')[0]).astype(int)\n    df['id_33_1'] = df['id_33'].apply(lambda x: x.split('x')[1]).astype(int)\n    df['id_33'] = np.where(df['id_33']=='0x0', np.nan, df['id_33'])\n\n    df['DeviceType'].map({'desktop':1, 'mobile':0})\n    return df\n\ntrain_identity = minify_identity_df(train_identity)\ntest_identity = minify_identity_df(test_identity)\n\nfor col in ['id_33']:\n    train_identity[col] = train_identity[col].fillna('unseen_before_label')\n    test_identity[col]  = test_identity[col].fillna('unseen_before_label')\n    \n    le = LabelEncoder()\n    le.fit(list(train_identity[col])+list(test_identity[col]))\n    train_identity[col] = le.transform(train_identity[col])\n    test_identity[col]  = le.transform(test_identity[col])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['DeviceInfo'].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"i_cols = ['card1','card2','card3','card5',\n          'C1','C2','C3','C4','C5','C6','C7','C8','C9','C10','C11','C12','C13','C14',\n          'D1','D2','D3','D4','D5','D6','D7','D8','D9',\n          'addr1','addr2',\n          'dist1','dist2',\n          'P_emaildomain', 'R_emaildomain'\n         ]\n\nfor col in i_cols:\n    temp_df = pd.concat([train_df[[col]], test_df[[col]]])\n    fq_encode = temp_df[col].value_counts().to_dict()   \n    train_df[col+'_fq_enc'] = train_df[col].map(fq_encode)\n    test_df[col+'_fq_enc']  = test_df[col].map(fq_encode)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3. Feature Engineering"},{"metadata":{},"cell_type":"markdown","source":"#### 3.1 TransactionDT"},{"metadata":{"trusted":true},"cell_type":"code","source":"START_DATE = datetime.datetime.strptime('2018-01-01', '%Y-%m-%d')\n\nfor df in [train_df, test_df]:\n    # Temporary\n    df['DT'] = df['TransactionDT'].apply(lambda x: START_DATE + (datetime.timedelta(seconds = x)))\n    df['DT_M'] = df['DT'].dt.month\n    df['DT_W'] = df['DT'].dt.weekofyear\n    df['DT_D'] = df['DT'].dt.dayofyear\n    \n    df['DT_hour'] = df['DT'].dt.hour\n    df['DT_day_week'] = df['DT'].dt.dayofweek\n    df['DT_day'] = df['DT'].dt.day\n    \ndel train_df['DT']\ndel test_df['DT']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 3.2 Transaction Amount"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['TransactionAmt_to_mean_ProductCD'] = train_df['TransactionAmt'] / train_df.groupby(['ProductCD'])['TransactionAmt'].transform('mean')\ntrain_df['TransactionAmt_to_mean_Hour'] = train_df['TransactionAmt'] / train_df.groupby(['DT_hour'])['TransactionAmt'].transform('mean')\ntrain_df['TransactionAmt_to_std_ProductCD'] = train_df['TransactionAmt'] / train_df.groupby(['ProductCD'])['TransactionAmt'].transform('std')\ntrain_df['TransactionAmt_to_std_Hour'] = train_df['TransactionAmt'] / train_df.groupby(['DT_hour'])['TransactionAmt'].transform('std')\n\ntest_df['TransactionAmt_to_mean_ProductCD'] = test_df['TransactionAmt'] / train_df.groupby(['ProductCD'])['TransactionAmt'].transform('mean')\ntest_df['TransactionAmt_to_mean_Hour'] = test_df['TransactionAmt'] / train_df.groupby(['DT_hour'])['TransactionAmt'].transform('mean')\ntest_df['TransactionAmt_to_std_ProductCD'] = test_df['TransactionAmt'] / train_df.groupby(['ProductCD'])['TransactionAmt'].transform('std')\ntest_df['TransactionAmt_to_std_Hour'] = test_df['TransactionAmt'] / train_df.groupby(['DT_hour'])['TransactionAmt'].transform('std')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 7. Model Running"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = reduce_mem_usage(train_df)\ntest_df = reduce_mem_usage(test_df)\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in list(train_df):\n    if train[col].dtype == 'O':\n        print(col)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Model Features \n## We can use set().difference() but order matters\nrm_cols = [\n    'TransactionID','TransactionDT',TARGET, 'M4', 'P_emaildomain', 'R_emaildomain'\n    #'TransactionID','TransactionDT',TARGET,'DeviceType', 'DeviceInfo', 'M4', 'P_emaildomain', 'R_emaildomain', 'id_30', 'id_31', 'id_34'\n]\nfeatures_columns = list(train_df)\nfor col in rm_cols:\n    if col in features_columns:\n        features_columns.remove(col)\n        \nfor icol in features_columns:\n    if train[icol].dtype == 'O':\n        print('bad col: ', col)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Model params\nlgb_params = {\n                    'objective':'binary',\n                    'boosting_type':'gbdt',\n                    'metric':'auc',\n                    'n_jobs':-1,\n                    'learning_rate':0.01,\n                    'num_leaves': 2**8,\n                    'max_depth':-1,\n                    'tree_learner':'serial',\n                    'colsample_bytree': 0.7,\n                    'subsample_freq':1,\n                    'subsample':1,\n                    'n_estimators':800,\n                    'max_bin':255,\n                    'verbose':-1,\n                    'seed': SEED,\n                    'early_stopping_rounds':100, \n                } ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"########################### Model\nimport lightgbm as lgb\n\ndef make_predictions(tr_df, tt_df, features_columns, target, lgb_params, NFOLDS):\n    folds = KFold(n_splits=NFOLDS, shuffle=True, random_state=SEED)\n\n    X,y = tr_df[features_columns], tr_df[target]    \n    P,P_y = tt_df[features_columns], tt_df[target]  \n\n    tt_df = tt_df[['TransactionID',target]]    \n    predictions = np.zeros(len(tt_df))\n    \n    for fold_, (trn_idx, val_idx) in enumerate(folds.split(X, y)):\n        print('Fold:',fold_)\n        tr_x, tr_y = X.iloc[trn_idx,:], y[trn_idx]\n        vl_x, vl_y = X.iloc[val_idx,:], y[val_idx]\n            \n        print(len(tr_x),len(vl_x))\n        tr_data = lgb.Dataset(tr_x, label=tr_y)\n\n        if LOCAL_TEST:\n            vl_data = lgb.Dataset(P, label=P_y) \n        else:\n            vl_data = lgb.Dataset(vl_x, label=vl_y)  \n\n        estimator = lgb.train(\n            lgb_params,\n            tr_data,\n            valid_sets = [tr_data, vl_data],\n            verbose_eval = 200,\n        )   \n        \n        pp_p = estimator.predict(P)\n        predictions += pp_p/NFOLDS\n\n        if LOCAL_TEST:\n            feature_imp = pd.DataFrame(sorted(zip(estimator.feature_importance(),X.columns)), columns=['Value','Feature'])\n            print(feature_imp)\n        \n        del tr_x, tr_y, vl_x, vl_y, tr_data, vl_data\n        gc.collect()\n        \n    tt_df['prediction']  = predictions\n    \n    return tt_df\n## -------------------","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"########################### Model Train\nif LOCAL_TEST:\n    test_predictions = make_predictions(train_df, test_df, features_columns, TARGET, lgb_params)\n    print(metrics.roc_auc_score(test_predictions[TARGET], test_predictions['prediction']))\nelse:\n    lgb_params['learning_rate'] = 0.01\n    lgb_params['n_estimators'] = 800\n    lgb_params['early_stopping_rounds'] = 100    \n    test_predictions = make_predictions(train_df, test_df, features_columns, TARGET, lgb_params, 10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## OUTPUT\nif not LOCAL_TEST:\n    test_predictions['isFraud'] = test_predictions['prediction']\n    test_predictions[['TransactionID','isFraud']].to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn import base\nfrom sklearn.model_selection import KFold\n\nclass KFoldTargetEncoderTrain(base.BaseEstimator, base.TransformerMixin):\n\n    def __init__(self, colnames,targetName,n_fold=5,verbosity=True,discardOriginal_col=False):\n\n        self.colnames = colnames\n        self.targetName = targetName\n        self.n_fold = n_fold\n        self.verbosity = verbosity\n        self.discardOriginal_col = discardOriginal_col\n\n    def fit(self, X, y=None):\n        return self\n\n\n    def transform(self,X):\n\n        assert(type(self.targetName) == str)\n        assert(type(self.colnames) == str)\n        assert(self.colnames in X.columns)\n        assert(self.targetName in X.columns)\n\n        mean_of_target = X[self.targetName].mean()\n        kf = KFold(n_splits = self.n_fold, shuffle = False, random_state=2019)\n\n\n\n        col_mean_name = self.colnames + '_' + 'Kfold_Target_Enc'\n        X[col_mean_name] = np.nan\n\n        for tr_ind, val_ind in kf.split(X):\n            X_tr, X_val = X.iloc[tr_ind], X.iloc[val_ind]\n#             print(tr_ind,val_ind)\n            X.loc[X.index[val_ind], col_mean_name] = X_val[self.colnames].map(X_tr.groupby(self.colnames)[self.targetName].mean())\n\n        X[col_mean_name].fillna(mean_of_target, inplace = True)\n\n        if self.verbosity:\n\n            encoded_feature = X[col_mean_name].values\n            print('Correlation between the new feature, {} and, {} is {}.'.format(col_mean_name,\n                                                                                      self.targetName,\n                                                                                      np.corrcoef(X[self.targetName].values, encoded_feature)[0][1]))\n        if self.discardOriginal_col:\n            X = X.drop(self.targetName, axis=1)\n            \n\n        return X\n\n    \nclass KFoldTargetEncoderTest(base.BaseEstimator, base.TransformerMixin):\n    \n    def __init__(self,train,colNames,encodedName):\n        \n        self.train = train\n        self.colNames = colNames\n        self.encodedName = encodedName\n        \n        \n    def fit(self, X, y=None):\n        return self\n\n    def transform(self,X):\n\n\n        mean = self.train[[self.colNames,self.encodedName]].groupby(self.colNames).mean().reset_index() \n        \n        dd = {}\n        for index, row in mean.iterrows():\n            dd[row[self.colNames]] = row[self.encodedName]\n\n        #print(dd)\n        \n#        X[self.encodedName] = X[self.colNames]\n#        X = X.replace({self.encodedName: dd})\n        \n        X[self.encodedName] = X[self.colNames].map(dd)\n        self.train[self.encodedName] = self.train[self.colNames].map(dd) \n\n        return X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#target_encoding_list = ['ProductCD', 'card4', 'card6', 'M4', 'P_emaildomain', 'R_emaildomain']#, 'DeviceType']\n\n#for col in target_encoding_list:\n#    targetc = KFoldTargetEncoderTrain(col,'isFraud',n_fold=10)\n #   train_df = targetc.fit_transform(train_df)\n\n  #  test_targetc = KFoldTargetEncoderTest(train_df,col,col+'_Kfold_Target_Enc')\n   # test_targetc.fit_transform(test_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"########################### card4, card6, ProductCD\n#################################################################################\n# Converting Strings to ints(or floats if nan in column) using frequency encoding\n# We will be able to use these columns as category or as numerical feature\n\n#for col in ['card4', 'card6', 'ProductCD']:\n #   print('Encoding', col)\n #   temp_df = pd.concat([train_df[[col]], test_df[[col]]])\n #   col_encoded = temp_df[col].value_counts().to_dict()   \n #   train_df[col] = train_df[col].map(col_encoded)\n #   test_df[col]  = test_df[col].map(col_encoded)\n #   print(col_encoded)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"########################### M columns\n#################################################################################\n# Converting Strings to ints(or floats if nan in column)\n\n#for col in ['M1','M2','M3','M5','M6','M7','M8','M9']:\n#    train_df[col] = train_df[col].map({'T':1, 'F':0})\n#    test_df[col]  = test_df[col].map({'T':1, 'F':0})\n\n#for col in ['M4']:\n #   print('Encoding', col)\n #   temp_df = pd.concat([train_df[[col]], test_df[[col]]])\n #   col_encoded = temp_df[col].value_counts().to_dict()   \n #   train_df[col] = train_df[col].map(col_encoded)\n #   test_df[col]  = test_df[col].map(col_encoded)\n #   print(col_encoded)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"########################### Final Minification\n#################################################################################\n\n#train_df = reduce_mem_usage(train_df)\n#test_df  = reduce_mem_usage(test_df)\n\n#train_identity = reduce_mem_usage(train_identity)\n#test_identity  = reduce_mem_usage(test_identity)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"########################### Export\n#################################################################################\n\n#train_df.to_pickle('train_transaction.pkl')\n#test_df.to_pickle('test_transaction.pkl')\n\n#train_identity.to_pickle('train_identity.pkl')\n#test_identity.to_pickle('test_identity.pkl')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#SEED = 42\n#seed_everything(SEED)\n#LOCAL_TEST = False\n#TARGET = 'isFraud'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"########################### Reset values for \"noise\" card1\n#valid_card = train_df['card1'].value_counts()\n#valid_card = valid_card[valid_card>10]\n#valid_card = list(valid_card.index)\n    \n#train_df['card1'] = np.where(train_df['card1'].isin(valid_card), train_df['card1'], np.nan)\n#test_df['card1']  = np.where(test_df['card1'].isin(valid_card), test_df['card1'], np.nan)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## FREQUENCY ENCODING\n#i_cols = ['card1','card2','card3','card5',\n#          'C1','C2','C3','C4','C5','C6','C7','C8','C9','C10','C11','C12','C13','C14',\n#          'D1','D2','D3','D4','D5','D6','D7','D8','D9',\n#          'addr1','addr2',\n#          'dist1','dist2',\n#          'P_emaildomain', 'R_emaildomain'\n#         ]\n\n#for col in i_cols:\n#    temp_df = pd.concat([train_df[[col]], test_df[[col]]])\n#    fq_encode = temp_df[col].value_counts().to_dict()   \n#    train_df[col+'_fq_enc'] = train_df[col].map(fq_encode)\n#    test_df[col+'_fq_enc']  = test_df[col].map(fq_encode)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## ProductCD and M4 Target mean\n#for col in ['ProductCD','M4']:\n#    temp_dict = train_df.groupby([col])[TARGET].agg(['mean']).reset_index().rename(\n                                                  #      columns={'mean': col+'_target_mean'})\n#    temp_dict.index = temp_dict[col].values\n#    temp_dict = temp_dict[col+'_target_mean'].to_dict()\n\n#    train_df[col+'_target_mean'] = train_df[col].map(temp_dict)\n#    test_df[col+'_target_mean']  = test_df[col].map(temp_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Encode Str columns\n#for col in list(train_df):\n#    if train_df[col].dtype=='O':\n#        print(col)\n#        train_df[col] = train_df[col].fillna('unseen_before_label')\n#        test_df[col]  = test_df[col].fillna('unseen_before_label')\n        \n#        train_df[col] = train_df[col].astype(str)\n#        test_df[col] = test_df[col].astype(str)\n        \n#        le = LabelEncoder()\n#        le.fit(list(train_df[col])+list(test_df[col]))\n#        train_df[col] = le.transform(train_df[col])\n#        test_df[col]  = le.transform(test_df[col])\n        \n#        train_df[col] = train_df[col].astype('category')\n#        test_df[col] = test_df[col].astype('category')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}