{"cells":[{"metadata":{},"cell_type":"markdown","source":"#### My own functions"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_hist(df, col, label):\n    plt.hist(df[col], label = label)\n    plt.legend()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1. Data Preparation"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# General imports\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nimport os, sys, gc, warnings, random, datetime\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split, KFold\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm\n\nimport math\n\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"########################### Helpers\n#################################################################################\n## -------------------\n## Seeder\n# :seed to make all processes deterministic     # type: int\ndef seed_everything(seed=0):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n## ------------------- \n\n## -------------------\n## Memory Reducer\n# :df pandas dataframe to reduce size             # type: pd.DataFrame()\n# :verbose                                        # type: bool\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df\n## -------------------","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"########################### Vars\n#################################################################################\nSEED = 42\nseed_everything(SEED)\nLOCAL_TEST = False\nTARGET = 'isFraud'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"########################### DATA LOAD\n#################################################################################\nprint('Load Data')\ntrain_df = pd.read_csv('../input/ieee-fraud-detection/train_transaction.csv')\ntest_df = pd.read_csv('../input/ieee-fraud-detection/test_transaction.csv')\ntest_df['isFraud'] = 0\n\ntrain_identity = pd.read_csv('../input/ieee-fraud-detection/train_identity.csv')\ntest_identity = pd.read_csv('../input/ieee-fraud-detection/test_identity.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"########################### Base check\n#################################################################################\n\nif LOCAL_TEST:\n    for df2 in [train_df, test_df, train_identity, test_identity]:\n        df = reduce_mem_usage(df2)\n\n        for col in list(df):\n            if not df[col].equals(df2[col]):\n                print('Bad transformation', col)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"########################### Base Minification\n#################################################################################\n\ntrain_df = reduce_mem_usage(train_df)\ntest_df  = reduce_mem_usage(test_df)\n\ntrain_identity = reduce_mem_usage(train_identity)\ntest_identity  = reduce_mem_usage(test_identity)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"base_columns = list(train_df) + list(train_identity)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"########################### Columns\n#################################################################################\n## Main Data\n# 'TransactionID',\n# 'isFraud',\n# 'TransactionDT',\n# 'TransactionAmt',\n# 'ProductCD',\n# 'card1' - 'card6',\n# 'addr1' - 'addr2',\n# 'dist1' - 'dist2',\n# 'P_emaildomain' - 'R_emaildomain',\n# 'C1' - 'C14'\n# 'D1' - 'D15'\n# 'M1' - 'M9'\n# 'V1' - 'V339'\n\n## Identity Data\n# 'TransactionID'\n# 'id_01' - 'id_38'\n# 'DeviceType',\n# 'DeviceInfo'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2. Data Preprocessing"},{"metadata":{},"cell_type":"markdown","source":"**Dataset Columns**\n\n**Transaction Data**\n\n'TransactionID'\n\n'isFraud'\n\n'TransactionDT'\n\n'TransactionAmt'\n\n'ProductCD' **Categorical Features**\n\n'card1' - 'card6' **Categorical Features**\n\n'addr1' - 'addr2' **Categorical Features**\n\n'dist1' - 'dist2'\n\n'P_emaildomain' **Categorical Features**\n\n'R_emaildomain' **Categorical Features**\n\n'C1' - 'C14'\n\n'D1' - 'D15'\n\n'M1' - 'M9' **Categorical Features**\n\n'V1' - 'V339'\n\n\n\n\n**Identity Data**\n\n'TransactionID'\n\n'id_01' - 'id_38' **Categorical Features id_12 - id_38**\n\n'DeviceType' **Categorical Features**\n\n'DeviceInfo' **Categorical Features**"},{"metadata":{},"cell_type":"markdown","source":"#### 2.1 Product CD"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,6))\n\ntotal = len(train_df)\n\nplt.subplot(121)\ng = sns.countplot(x = 'ProductCD', data = train_df)\ng.set_title('ProductCD Distribution', fontsize = 15)\ng.set_xlabel(\"Product Code\", fontsize=15)\ng.set_ylabel(\"Count\", fontsize=15)\nfor p in g.patches:\n    height = p.get_height()\n    g.text(p.get_x()+p.get_width()/2.,\n            height + 3,\n            '{:1.2f}%'.format(height/total*100),\n            ha=\"center\", fontsize=14) \n\nplt.subplot(122)\ng1 = sns.countplot(x='ProductCD', hue='isFraud', data=train_df)\ng1.set_title('ProductCD by Fraud', fontsize = 15)\ng1.set_xlabel(\"Product Code\", fontsize=15)\ng1.set_ylabel(\"Count\", fontsize=15)\nplt.legend(title='Fraud', loc='best', labels=['No', 'Yes'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[train_df['isFraud'] == 1]['ProductCD'].value_counts(normalize = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (12,12))\ntemp = train_df.groupby('ProductCD')['isFraud'].value_counts(normalize = True).unstack()\na = temp.plot.bar(stacked = True)\na.set_title('Rate of Fraud by Product Code', fontsize = 15)\nplt.xticks(rotation = 'horizontal')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These results susgest that with product code, we need to do 2 things:\n* Encode categorical levels using Frequency Encoding\n* Add target mean by categorical level. It is found here that product C has highest fraud rate, compared to others"},{"metadata":{"trusted":true},"cell_type":"code","source":"# ProductCD Frequency Encoding\ncol = 'ProductCD'\ntemp_df = pd.concat([train_df[[col]], test_df[[col]]])   # I don't want to use test data\n#temp_df = train_df[[col]]\ncol_encoded = temp_df[col].value_counts().to_dict()\ntrain_df[col] = train_df[col].map(col_encoded)\ntest_df[col]  = test_df[col].map(col_encoded)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## ProductCD Target mean\nfor col in ['ProductCD']:\n    temp_dict = train_df.groupby([col])[TARGET].agg(['mean']).reset_index().rename(\n                                                        columns={'mean': col+'_target_mean'})\n    print(temp_dict)\n    print('=============')\n    \n    temp_dict.index = temp_dict[col].values\n    print(temp_dict.index)\n    print('=============')\n    \n    temp_dict = temp_dict[col+'_target_mean'].to_dict()\n    print(temp_dict)\n    print('****************')\n    \n    train_df[col+'_target_mean'] = train_df[col].map(temp_dict)\n    test_df[col+'_target_mean']  = test_df[col].map(temp_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[['ProductCD', 'ProductCD_target_mean']].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.2 Card1 - Card6"},{"metadata":{},"cell_type":"markdown","source":"#### a. Card 1, 2, 3, 5\n\nThe card 1,2,3, and 5 was represented as numerical values, temping us to plot the histogram. However, we need to remember that card columns were classified as categorical variables. Meaning it's likely that these numerical variables were coded for categorical variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.describe().loc[:,'card1':'card5']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.loc[:,'card1':'card5'].nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Card 1 contains 13553 unique values, suggesting card 1 may have been ID of the card. Card 2,3 and 5 have less unique values, so perhaps they could be expiration date, or combinations that generate card identity? Since we don't know how these information was scrammbled, we might pickup patterns generated by encryption algorithm instead of data. We can calculate target mean grouped by 'card1' - 'card5' (except 'card4'"},{"metadata":{"trusted":true},"cell_type":"code","source":"#for col in ['card1', 'card2', 'card3', 'card5']:\n#    temp_dict = train_df.groupby([col])[TARGET].agg(['mean']).reset_index().rename(\n#                                                        columns={'mean': col+'_target_mean'})\n#    temp_dict.index = temp_dict[col].values\n#    temp_dict = temp_dict[col+'_target_mean'].to_dict()\n\n#    train_df[col+'_target_mean'] = train_df[col].map(temp_dict)\n#    test_df[col+'_target_mean']  = test_df[col].map(temp_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"########################### Reset values for \"noise\" card1\ni_cols = ['card1']\n\nfor col in i_cols: \n    valid_card = pd.concat([train_df[[col]], test_df[[col]]])\n    valid_card = valid_card[col].value_counts()\n    valid_card = valid_card[valid_card>2]\n    valid_card = list(valid_card.index)\n\n    train_df[col] = np.where(train_df[col].isin(test_df[col]), train_df[col], np.nan)\n    test_df[col]  = np.where(test_df[col].isin(train_df[col]), test_df[col], np.nan)\n\n    train_df[col] = np.where(train_df[col].isin(valid_card), train_df[col], np.nan)\n    test_df[col]  = np.where(test_df[col].isin(valid_card), test_df[col], np.nan)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"b. Card4"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,6))\n\ntotal = len(train_df)\n\nplt.subplot(121)\ng = sns.countplot(x = 'card4', data = train_df)\ng.set_title('Card Network Distribution', fontsize = 15)\ng.set_xlabel(\"Card Issuers\", fontsize=15)\ng.set_ylabel(\"Count\", fontsize=15)\nfor p in g.patches:\n    height = p.get_height()\n    g.text(p.get_x()+p.get_width()/2.,\n            height + 3,\n            '{:1.2f}%'.format(height/total*100),\n            ha=\"center\", fontsize=14) \n\nplt.subplot(122)\ng1 = sns.countplot(x='card4', hue='isFraud', data=train_df)\ng1.set_title('Card Network by Fraud', fontsize = 15)\ng1.set_xlabel(\"Card Issuers\", fontsize=15)\ng1.set_ylabel(\"Count\", fontsize=15)\nplt.legend(title='Fraud', loc='best', labels=['No', 'Yes'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# visualization of table\nplt.figure(figsize=(12,12))\nb = train_df.groupby('card4')['isFraud'].value_counts(normalize = True).unstack().plot.bar(stacked = True)\nb.set_title('Rate of Fraud by Card Network', fontsize = 15)\nplt.xticks(rotation='horizontal')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Card4 Frequency Encoding\ncol = 'card4'\ntemp_df = pd.concat([train_df[[col]], test_df[[col]]])   # I don't want to use test data\n#temp_df = train_df[[col]]\ncol_encoded = temp_df[col].value_counts().to_dict()\ntrain_df[col] = train_df[col].map(col_encoded)\ntest_df[col]  = test_df[col].map(col_encoded)\n\n## Card4 Target mean\n#for col in ['card4']:\n#    temp_dict = train_df.groupby([col])[TARGET].agg(['mean']).reset_index().rename(\n#                                                        columns={'mean': col+'_target_mean'})\n#    temp_dict.index = temp_dict[col].values\n#    temp_dict = temp_dict[col+'_target_mean'].to_dict()\n\n#    train_df[col+'_target_mean'] = train_df[col].map(temp_dict)\n#    test_df[col+'_target_mean']  = test_df[col].map(temp_dict)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"c. Card6"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,6))\n\ntotal = len(train_df)\n\nplt.subplot(121)\ng = sns.countplot(x = 'card6', data = train_df)\ng.set_title('Card Type Distribution', fontsize = 15)\ng.set_xlabel(\"Card Type\", fontsize=15)\ng.set_ylabel(\"Count\", fontsize=15)\nfor p in g.patches:\n    height = p.get_height()\n    g.text(p.get_x()+p.get_width()/2.,\n            height + 3,\n            '{:1.2f}%'.format(height/total*100),\n            ha=\"center\", fontsize=14) \n\nplt.subplot(122)\ng1 = sns.countplot(x='card6', hue='isFraud', data=train_df)\ng1.set_title('Card Type by Fraud', fontsize = 15)\ng1.set_xlabel(\"Card Type\", fontsize=15)\ng1.set_ylabel(\"Count\", fontsize=15)\nplt.legend(title='Fraud', loc='best', labels=['No', 'Yes'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# visualization of table\nplt.figure(figsize=(12,12))\nc = train_df.groupby('card6')['isFraud'].value_counts(normalize = True).unstack().plot.bar(stacked = True)\nc.set_title('Rate of Fraud by Card Type', fontsize = 15)\nplt.xticks(rotation='horizontal')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Credit card has much higher fraud rate as compared with other types of card"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Card6 Frequency Encoding\ncol = 'card6'\ntemp_df = pd.concat([train_df[[col]], test_df[[col]]])   # I don't want to use test data\n#temp_df = train_df[[col]]\ncol_encoded = temp_df[col].value_counts().to_dict()\ntrain_df[col] = train_df[col].map(col_encoded)\ntest_df[col]  = test_df[col].map(col_encoded)\n\n## Card6 Target mean\n#for col in ['card6']:\n#    temp_dict = train_df.groupby([col])[TARGET].agg(['mean']).reset_index().rename(\n#                                                        columns={'mean': col+'_target_mean'})\n#    temp_dict.index = temp_dict[col].values\n#    temp_dict = temp_dict[col+'_target_mean'].to_dict()\n\n#    train_df[col+'_target_mean'] = train_df[col].map(temp_dict)\n#    test_df[col+'_target_mean']  = test_df[col].map(temp_dict)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.3 Addr1 - Addr2"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_hist(train_df, 'addr1', 'addr1')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_hist(train_df, 'addr2', 'addr2')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(x='addr1', y ='isFraud', data = train_df)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Addr1 - Addr2 Target mean\n#for col in ['addr1', 'addr2']:\n#    temp_dict = train_df.groupby([col])[TARGET].agg(['mean']).reset_index().rename(\n#                                                        columns={'mean': col+'_target_mean'})\n#    temp_dict.index = temp_dict[col].values\n#    temp_dict = temp_dict[col+'_target_mean'].to_dict()\n\n#    train_df[col+'_target_mean'] = train_df[col].map(temp_dict)\n#    test_df[col+'_target_mean']  = test_df[col].map(temp_dict)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.4 Email domain"},{"metadata":{"trusted":true},"cell_type":"code","source":"#train_df[\"P_parent_emaildomain\"] = train_df[\"P_emaildomain\"].str.split('.', expand = True)[[0]]\n#train_df[\"R_parent_emaildomain\"] = train_df[\"R_emaildomain\"].str.split('.', expand = True)[[0]]\n\n#test_df[\"P_parent_emaildomain\"] = test_df[\"P_emaildomain\"].str.split('.', expand = True)[[0]]\n#test_df[\"R_parent_emaildomain\"] = test_df[\"R_emaildomain\"].str.split('.', expand = True)[[0]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# P-emaildomain & R-emaildomain Frequency Encoding\n#for col in ['P_parent_emaildomain', 'R_parent_emaildomain']:\n#    temp_df = pd.concat([train_df[[col]], test_df[[col]]])   # I don't want to use test data\n    #temp_df = train_df[[col]]\n#    col_encoded = temp_df[col].value_counts().to_dict()\n#    train_df[col] = train_df[col].map(col_encoded)\n#    test_df[col]  = test_df[col].map(col_encoded)\n\n## P-emaildomain & R-emaildomain Target mean\n#for col in ['P_parent_emaildomain', 'R_parent_emaildomain']:\n#    temp_dict = train_df.groupby([col])[TARGET].agg(['mean']).reset_index().rename(\n#                                                        columns={'mean': col+'_target_mean'})\n#    temp_dict.index = temp_dict[col].values\n#    temp_dict = temp_dict[col+'_target_mean'].to_dict()\n\n#    train_df[col+'_target_mean'] = train_df[col].map(temp_dict)\n#    test_df[col+'_target_mean']  = test_df[col].map(temp_dict)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.5 'M1' - 'M9'"},{"metadata":{"trusted":true},"cell_type":"code","source":"#temp = train_df[train_df['isFraud']==True].groupby('M4')['isFraud'].value_counts(normalize=True)\ntemp = train_df.groupby('M4')['isFraud'].value_counts(normalize = True)\ntemp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['M4'].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 'M4' Target mean\nfor col in ['M4']:\n    temp_dict = train_df.groupby([col])[TARGET].agg(['mean']).reset_index().rename(\n                                                        columns={'mean': col+'_target_mean'})\n    temp_dict.index = temp_dict[col].values\n    temp_dict = temp_dict[col+'_target_mean'].to_dict()\n\n    train_df[col+'_target_mean'] = train_df[col].map(temp_dict)\n    test_df[col+'_target_mean']  = test_df[col].map(temp_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in ['M1','M2','M3','M5','M6','M7','M8','M9']:\n    train_df[col] = train_df[col].map({'T':1, 'F':0})\n    test_df[col]  = test_df[col].map({'T':1, 'F':0})\n    \nfor col in ['M4']:\n    print('Encoding', col)\n    temp_df = pd.concat([train_df[[col]], test_df[[col]]])\n    col_encoded = temp_df[col].value_counts().to_dict()   \n    train_df[col] = train_df[col].map(col_encoded)\n    test_df[col]  = test_df[col].map(col_encoded)\n    print(col_encoded)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's combine the data and work with the whole dataset\n\n#train = pd.merge(train_df, train_identity, on='TransactionID', how='left')\n#test = pd.merge(test_df, test_identity, on='TransactionID', how='left')\n#del train_identity, train_df, test_identity, test_df\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Reduce Memory Usage\n\n#train = reduce_mem_usage(train)\n#test = reduce_mem_usage(test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.6 'id_12' - 'id_38'"},{"metadata":{"trusted":true},"cell_type":"code","source":"#train['id_23'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate Target Mean grouped by 'id_23'\n## 'id_23' Target mean\n#for col in ['id_23']:\n#    temp_dict = train.groupby([col])[TARGET].agg(['mean']).reset_index().rename(\n#                                                        columns={'mean': col+'_target_mean'})\n#    temp_dict.index = temp_dict[col].values\n#    temp_dict = temp_dict[col+'_target_mean'].to_dict()\n\n#    train[col] = train[col].map(temp_dict)\n#    test[col]  = test[col].map(temp_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"########################### Identity columns\n#################################################################################\n\ndef minify_identity_df(df):\n\n    df['id_12'] = df['id_12'].map({'Found':1, 'NotFound':0})\n    df['id_15'] = df['id_15'].map({'New':2, 'Found':1, 'Unknown':0})\n    df['id_16'] = df['id_16'].map({'Found':1, 'NotFound':0})\n\n    df['id_23'] = df['id_23'].map({'TRANSPARENT':4, 'IP_PROXY':3, 'IP_PROXY:ANONYMOUS':2, 'IP_PROXY:HIDDEN':1})\n\n    df['id_27'] = df['id_27'].map({'Found':1, 'NotFound':0})\n    df['id_28'] = df['id_28'].map({'New':2, 'Found':1})\n\n    df['id_29'] = df['id_29'].map({'Found':1, 'NotFound':0})\n\n    df['id_35'] = df['id_35'].map({'T':1, 'F':0})\n    df['id_36'] = df['id_36'].map({'T':1, 'F':0})\n    df['id_37'] = df['id_37'].map({'T':1, 'F':0})\n    df['id_38'] = df['id_38'].map({'T':1, 'F':0})\n\n    df['id_34'] = df['id_34'].fillna(':0')\n    df['id_34'] = df['id_34'].apply(lambda x: x.split(':')[1]).astype(np.int8)\n    df['id_34'] = np.where(df['id_34']==0, np.nan, df['id_34'])\n    \n    df['id_33'] = df['id_33'].fillna('0x0')\n    df['id_33_0'] = df['id_33'].apply(lambda x: x.split('x')[0]).astype(int)\n    df['id_33_1'] = df['id_33'].apply(lambda x: x.split('x')[1]).astype(int)\n    df['id_33'] = np.where(df['id_33']=='0x0', np.nan, df['id_33'])\n\n    df['DeviceType'].map({'desktop':1, 'mobile':0})\n    return df\n\ntrain_identity = minify_identity_df(train_identity)\ntest_identity = minify_identity_df(test_identity)\n\nfor col in ['id_33']:\n    train_identity[col] = train_identity[col].fillna('unseen_before_label')\n    test_identity[col]  = test_identity[col].fillna('unseen_before_label')\n    \n    le = LabelEncoder()\n    le.fit(list(train_identity[col])+list(test_identity[col]))\n    train_identity[col] = le.transform(train_identity[col])\n    test_identity[col]  = le.transform(test_identity[col])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3. Feature Engineering"},{"metadata":{},"cell_type":"markdown","source":"#### 3.1 TransactionDT"},{"metadata":{"trusted":true},"cell_type":"code","source":"START_DATE = datetime.datetime.strptime('2018-01-01', '%Y-%m-%d')\n\nfor df in [train_df, test_df]:\n    # Temporary\n    df['DT'] = df['TransactionDT'].apply(lambda x: START_DATE + (datetime.timedelta(seconds = x)))\n    df['DT_M'] = df['DT'].dt.month\n    df['DT_W'] = df['DT'].dt.weekofyear\n    df['DT_D'] = df['DT'].dt.dayofyear\n    \n    df['DT_hour'] = df['DT'].dt.hour\n    df['DT_day_week'] = df['DT'].dt.dayofweek\n    df['DT_day'] = df['DT'].dt.day\n    \n    # D9 column\n    df['D9'] = np.where(df['D9'].isna(),0,1)\n\n#del train_df['DT']\n#del test_df['DT']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 3.2 Transaction Amount"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['TransactionAmt_to_mean_ProductCD'] = train_df['TransactionAmt'] / train_df.groupby(['ProductCD'])['TransactionAmt'].transform('mean')\ntrain_df['TransactionAmt_to_mean_Hour'] = train_df['TransactionAmt'] / train_df.groupby(['DT_hour'])['TransactionAmt'].transform('mean')\ntrain_df['TransactionAmt_to_std_ProductCD'] = train_df['TransactionAmt'] / train_df.groupby(['ProductCD'])['TransactionAmt'].transform('std')\ntrain_df['TransactionAmt_to_std_Hour'] = train_df['TransactionAmt'] / train_df.groupby(['DT_hour'])['TransactionAmt'].transform('std')\n\ntest_df['TransactionAmt_to_mean_ProductCD'] = test_df['TransactionAmt'] / train_df.groupby(['ProductCD'])['TransactionAmt'].transform('mean')\ntest_df['TransactionAmt_to_mean_Hour'] = test_df['TransactionAmt'] / train_df.groupby(['DT_hour'])['TransactionAmt'].transform('mean')\ntest_df['TransactionAmt_to_std_ProductCD'] = test_df['TransactionAmt'] / train_df.groupby(['ProductCD'])['TransactionAmt'].transform('std')\ntest_df['TransactionAmt_to_std_Hour'] = test_df['TransactionAmt'] / train_df.groupby(['DT_hour'])['TransactionAmt'].transform('std')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's add some kind of client uID based on cardID ad addr columns\n# The value will be very specific for each client so we need to remove it\n# from final feature. But we can use it for aggregations.\ntrain_df['uid'] = train_df['card1'].astype(str)+'_'+train_df['card2'].astype(str)\ntest_df['uid'] = test_df['card1'].astype(str)+'_'+test_df['card2'].astype(str)\n\ntrain_df['uid2'] = train_df['uid'].astype(str)+'_'+train_df['card3'].astype(str)+'_'+train_df['card4'].astype(str)\ntest_df['uid2'] = test_df['uid'].astype(str)+'_'+test_df['card3'].astype(str)+'_'+test_df['card4'].astype(str)\n\ntrain_df['uid3'] = train_df['uid2'].astype(str)+'_'+train_df['addr1'].astype(str)+'_'+train_df['addr2'].astype(str)\ntest_df['uid3'] = test_df['uid2'].astype(str)+'_'+test_df['addr1'].astype(str)+'_'+test_df['addr2'].astype(str)\n\n# Check if the Transaction Amount is common or not (we can use freq encoding here)\n# In our dialog with a model we are telling to trust or not to these values   \ntrain_df['TransactionAmt_check'] = np.where(train_df['TransactionAmt'].isin(test_df['TransactionAmt']), 1, 0)\ntest_df['TransactionAmt_check']  = np.where(test_df['TransactionAmt'].isin(train_df['TransactionAmt']), 1, 0)\n\n#\ni_cols = ['card1','card2','card3','card5','uid','uid2','uid3']\n\nfor col in i_cols:\n    for agg_type in ['mean','std']:\n        new_col_name = col+'_TransactionAmt_'+agg_type\n        temp_df = pd.concat([train_df[[col, 'TransactionAmt']], test_df[[col,'TransactionAmt']]])\n        #temp_df['TransactionAmt'] = temp_df['TransactionAmt'].astype(int)\n        temp_df = temp_df.groupby([col])['TransactionAmt'].agg([agg_type]).reset_index().rename(\n                                                columns={agg_type: new_col_name})\n        \n        temp_df.index = list(temp_df[col])\n        temp_df = temp_df[new_col_name].to_dict()   \n    \n        train_df[new_col_name] = train_df[col].map(temp_df)\n        test_df[new_col_name]  = test_df[col].map(temp_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['TransactionAmt'] = np.log1p(train_df['TransactionAmt'])\ntest_df['TransactionAmt'] = np.log1p(test_df['TransactionAmt'])      ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 'P_emaildomain' - 'R_emaildomain'\np = 'P_emaildomain'\nr = 'R_emaildomain'\nuknown = 'email_not_provided'\n\nfor df in [train_df, test_df]:\n    df[p] = df[p].fillna(uknown)\n    df[r] = df[r].fillna(uknown)\n    \n    # Check if P_emaildomain matches R_emaildomain\n    df['email_check'] = np.where((df[p]==df[r])&(df[p]!=uknown),1,0)\n\n    df[p+'_prefix'] = df[p].apply(lambda x: x.split('.')[0])\n    df[r+'_prefix'] = df[r].apply(lambda x: x.split('.')[0])\n\n## Local test doesn't show any boost here, \n## but I think it's a good option for model stability \n\n## Also, we will do frequency encoding later","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"########################### Device info\nfor df in [train_identity, test_identity]:\n    ########################### Device info\n    df['DeviceInfo'] = df['DeviceInfo'].fillna('unknown_device').str.lower()\n    df['DeviceInfo_device'] = df['DeviceInfo'].apply(lambda x: ''.join([i for i in x if i.isalpha()]))\n    df['DeviceInfo_version'] = df['DeviceInfo'].apply(lambda x: ''.join([i for i in x if i.isnumeric()]))\n    \n    ########################### Device info 2\n    df['id_30'] = df['id_30'].fillna('unknown_device').str.lower()\n    df['id_30_device'] = df['id_30'].apply(lambda x: ''.join([i for i in x if i.isalpha()]))\n    df['id_30_version'] = df['id_30'].apply(lambda x: ''.join([i for i in x if i.isnumeric()]))\n    \n    ########################### Browser\n    df['id_31'] = df['id_31'].fillna('unknown_device').str.lower()\n    df['id_31_device'] = df['id_31'].apply(lambda x: ''.join([i for i in x if i.isalpha()]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"########################### Merge Identity columns\ntemp_df = train_df[['TransactionID']]\ntemp_df = temp_df.merge(train_identity, on=['TransactionID'], how='left')\ndel temp_df['TransactionID']\ntrain_df = pd.concat([train_df,temp_df], axis=1)\n    \ntemp_df = test_df[['TransactionID']]\ntemp_df = temp_df.merge(test_identity, on=['TransactionID'], how='left')\ndel temp_df['TransactionID']\ntest_df = pd.concat([test_df,temp_df], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 3.3 Frequency Encoding\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"########################### Freq encoding\ni_cols = ['card1','card2','card3','card5',\n          'C1','C2','C3','C4','C5','C6','C7','C8','C9','C10','C11','C12','C13','C14',\n          'D1','D2','D3','D4','D5','D6','D7','D8',\n          'addr1','addr2',\n          'dist1','dist2',\n          'P_emaildomain', 'R_emaildomain',\n          'DeviceInfo','DeviceInfo_device','DeviceInfo_version',\n          'id_30','id_30_device','id_30_version',\n          'id_31_device',\n          'id_33',\n          'uid','uid2','uid3',\n         ]\n\nfor col in i_cols:\n    temp_df = pd.concat([train_df[[col]], test_df[[col]]])\n    fq_encode = temp_df[col].value_counts(dropna=False).to_dict()   \n    train_df[col+'_fq_enc'] = train_df[col].map(fq_encode)\n    test_df[col+'_fq_enc']  = test_df[col].map(fq_encode)\n\n\nfor col in ['DT_M','DT_W','DT_D']:\n    temp_df = pd.concat([train_df[[col]], test_df[[col]]])\n    fq_encode = temp_df[col].value_counts().to_dict()\n            \n    train_df[col+'_total'] = train_df[col].map(fq_encode)\n    test_df[col+'_total']  = test_df[col].map(fq_encode)\n        \n\nperiods = ['DT_M','DT_W','DT_D']\ni_cols = ['uid']\nfor period in periods:\n    for col in i_cols:\n        new_column = col + '_' + period\n            \n        temp_df = pd.concat([train_df[[col,period]], test_df[[col,period]]])\n        temp_df[new_column] = temp_df[col].astype(str) + '_' + (temp_df[period]).astype(str)\n        fq_encode = temp_df[new_column].value_counts().to_dict()\n            \n        train_df[new_column] = (train_df[col].astype(str) + '_' + train_df[period].astype(str)).map(fq_encode)\n        test_df[new_column]  = (test_df[col].astype(str) + '_' + test_df[period].astype(str)).map(fq_encode)\n        \n        train_df[new_column] /= train_df[period+'_total']\n        test_df[new_column]  /= test_df[period+'_total']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"########################### M columns (except M4)\n# All these columns are binary encoded 1/0\n# We can have some features from it\ni_cols = ['M1','M2','M3','M5','M6','M7','M8','M9']\n\nfor df in [train_df, test_df]:\n    df['M_sum'] = df[i_cols].sum(axis=1).astype(np.int8)\n    df['M_na'] = df[i_cols].isna().sum(axis=1).astype(np.int8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"########################### Encode Str columns\n# For all such columns (probably not)\n# we already did frequency encoding (numeric feature)\n# so we will use astype('category') here\nfor col in list(train_df):\n    if train_df[col].dtype=='O':\n        print(col)\n        train_df[col] = train_df[col].fillna('unseen_before_label')\n        test_df[col]  = test_df[col].fillna('unseen_before_label')\n        \n        train_df[col] = train_df[col].astype(str)\n        test_df[col] = test_df[col].astype(str)\n        \n        le = LabelEncoder()\n        le.fit(list(train_df[col])+list(test_df[col]))\n        train_df[col] = le.transform(train_df[col])\n        test_df[col]  = le.transform(test_df[col])\n        \n        train_df[col] = train_df[col].astype('category')\n        test_df[col] = test_df[col].astype('category')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 7. Model Running"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = reduce_mem_usage(train_df)\ntest_df = reduce_mem_usage(test_df)\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in list(train_df):\n    if train_df[col].dtype == 'O':\n        print(col)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Encode Str columns\nfor col in list(train_df):\n    if train_df[col].dtype=='O':\n        print(col)\n        train_df[col] = train_df[col].fillna('unseen_before_label')\n        test_df[col]  = test_df[col].fillna('unseen_before_label')\n        \n        train_df[col] = train_df[col].astype(str)\n        test_df[col] = test_df[col].astype(str)\n        \n        le = LabelEncoder()\n        le.fit(list(train_df[col])+list(test_df[col]))\n        train_df[col] = le.transform(train_df[col])\n        test_df[col]  = le.transform(test_df[col])\n        print(train_df[col].head())\n        \n        train_df[col] = train_df[col].astype('category')\n        test_df[col] = test_df[col].astype('category')\n        print(train_df[col].head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"########################### Model Features \n## We can use set().difference() but the order matters\n## Matters only for deterministic results\n## In case of remove() we will not change order\n## even if variable will be renamed\n## please see this link to see how set is ordered\n## https://stackoverflow.com/questions/12165200/order-of-unordered-python-sets\nrm_cols = [\n    'TransactionID','TransactionDT', # These columns are pure noise right now\n    TARGET,                          # Not target in features))\n    'uid','uid2','uid3',             # Our new client uID -> very noisy data\n    'bank_type',                     # Victims bank could differ by time\n    'DT','DT_M','DT_W','DT_D',       # Temporary Variables\n    'DT_hour','DT_day_week','DT_day',\n    'DT_D_total','DT_W_total','DT_M_total',\n    'id_30','id_31','id_33',\n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"########################### Features elimination \nfrom scipy.stats import ks_2samp\nfeatures_check = []\ncolumns_to_check = set(list(train_df)).difference(base_columns+rm_cols)\nfor i in columns_to_check:\n    features_check.append(ks_2samp(test_df[i], train_df[i])[1])\n\nfeatures_check = pd.Series(features_check, index=columns_to_check).sort_values() \nfeatures_discard = list(features_check[features_check==0].index)\nprint(features_discard)\n\n# We will reset this list for now,\n# Good droping will be in other kernels\n# with better checking\nfeatures_discard = [] \n\n# Final features list\nfeatures_columns = [col for col in list(train_df) if col not in rm_cols + features_discard]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ========== Model params\nlgb_params = {\n                    'objective':'binary',\n                    'boosting_type':'gbdt',\n                    'metric':'auc',\n                    'n_jobs':-1,\n                    'learning_rate':0.01,\n                    'num_leaves': 2**8,\n                    'max_depth':-1,\n                    'tree_learner':'serial',\n                    'colsample_bytree': 0.7,\n                    'subsample_freq':1,\n                    'subsample':0.7,\n                    'n_estimators':800,\n                    'max_bin':255,\n                    'verbose':-1,\n                    'seed': SEED,\n                    'early_stopping_rounds':100, \n                } ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"########################### Model\nimport lightgbm as lgb\n\ndef make_predictions(tr_df, tt_df, features_columns, target, lgb_params, NFOLDS):\n    folds = KFold(n_splits=NFOLDS, shuffle=True, random_state=SEED)\n\n    X,y = tr_df[features_columns], tr_df[target]    \n    P,P_y = tt_df[features_columns], tt_df[target]  \n\n    tt_df = tt_df[['TransactionID',target]]    \n    predictions = np.zeros(len(tt_df))\n    \n    for fold_, (trn_idx, val_idx) in enumerate(folds.split(X, y)):\n        print('Fold:',fold_)\n        tr_x, tr_y = X.iloc[trn_idx,:], y[trn_idx]\n        vl_x, vl_y = X.iloc[val_idx,:], y[val_idx]\n            \n        print(len(tr_x),len(vl_x))\n        tr_data = lgb.Dataset(tr_x, label=tr_y)\n\n        if LOCAL_TEST:\n            vl_data = lgb.Dataset(P, label=P_y) \n        else:\n            vl_data = lgb.Dataset(vl_x, label=vl_y)  \n\n        estimator = lgb.train(\n            lgb_params,\n            tr_data,\n            valid_sets = [tr_data, vl_data],\n            verbose_eval = 200,\n        )   \n        \n        pp_p = estimator.predict(P)\n        predictions += pp_p/NFOLDS\n\n        if LOCAL_TEST:\n            feature_imp = pd.DataFrame(sorted(zip(estimator.feature_importance(),X.columns)), columns=['Value','Feature'])\n            print(feature_imp)\n        \n        del tr_x, tr_y, vl_x, vl_y, tr_data, vl_data\n        gc.collect()\n        \n    tt_df['prediction']  = predictions\n    \n    return tt_df\n## -------------------","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"########################### Model Train\nif LOCAL_TEST:\n    test_predictions = make_predictions(train_df, test_df, features_columns, TARGET, lgb_params)\n    print(metrics.roc_auc_score(test_predictions[TARGET], test_predictions['prediction']))\nelse:\n    lgb_params['learning_rate'] = 0.01\n    lgb_params['n_estimators'] = 800\n    lgb_params['early_stopping_rounds'] = 100    \n    test_predictions = make_predictions(train_df, test_df, features_columns, TARGET, lgb_params, 10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## OUTPUT\nif not LOCAL_TEST:\n    test_predictions['isFraud'] = test_predictions['prediction']\n    test_predictions[['TransactionID','isFraud']].to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#target_encoding_list = ['ProductCD', 'card4', 'card6', 'M4', 'P_emaildomain', 'R_emaildomain']#, 'DeviceType']\n\n#for col in target_encoding_list:\n#    targetc = KFoldTargetEncoderTrain(col,'isFraud',n_fold=10)\n #   train_df = targetc.fit_transform(train_df)\n\n  #  test_targetc = KFoldTargetEncoderTest(train_df,col,col+'_Kfold_Target_Enc')\n   # test_targetc.fit_transform(test_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"########################### card4, card6, ProductCD\n#################################################################################\n# Converting Strings to ints(or floats if nan in column) using frequency encoding\n# We will be able to use these columns as category or as numerical feature\n\n#for col in ['card4', 'card6', 'ProductCD']:\n #   print('Encoding', col)\n #   temp_df = pd.concat([train_df[[col]], test_df[[col]]])\n #   col_encoded = temp_df[col].value_counts().to_dict()   \n #   train_df[col] = train_df[col].map(col_encoded)\n #   test_df[col]  = test_df[col].map(col_encoded)\n #   print(col_encoded)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"########################### M columns\n#################################################################################\n# Converting Strings to ints(or floats if nan in column)\n\n#for col in ['M1','M2','M3','M5','M6','M7','M8','M9']:\n#    train_df[col] = train_df[col].map({'T':1, 'F':0})\n#    test_df[col]  = test_df[col].map({'T':1, 'F':0})\n\n#for col in ['M4']:\n #   print('Encoding', col)\n #   temp_df = pd.concat([train_df[[col]], test_df[[col]]])\n #   col_encoded = temp_df[col].value_counts().to_dict()   \n #   train_df[col] = train_df[col].map(col_encoded)\n #   test_df[col]  = test_df[col].map(col_encoded)\n #   print(col_encoded)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"########################### Final Minification\n#################################################################################\n\n#train_df = reduce_mem_usage(train_df)\n#test_df  = reduce_mem_usage(test_df)\n\n#train_identity = reduce_mem_usage(train_identity)\n#test_identity  = reduce_mem_usage(test_identity)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"########################### Export\n#################################################################################\n\n#train_df.to_pickle('train_transaction.pkl')\n#test_df.to_pickle('test_transaction.pkl')\n\n#train_identity.to_pickle('train_identity.pkl')\n#test_identity.to_pickle('test_identity.pkl')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#SEED = 42\n#seed_everything(SEED)\n#LOCAL_TEST = False\n#TARGET = 'isFraud'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"########################### Reset values for \"noise\" card1\n#valid_card = train_df['card1'].value_counts()\n#valid_card = valid_card[valid_card>10]\n#valid_card = list(valid_card.index)\n    \n#train_df['card1'] = np.where(train_df['card1'].isin(valid_card), train_df['card1'], np.nan)\n#test_df['card1']  = np.where(test_df['card1'].isin(valid_card), test_df['card1'], np.nan)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## FREQUENCY ENCODING\n#i_cols = ['card1','card2','card3','card5',\n#          'C1','C2','C3','C4','C5','C6','C7','C8','C9','C10','C11','C12','C13','C14',\n#          'D1','D2','D3','D4','D5','D6','D7','D8','D9',\n#          'addr1','addr2',\n#          'dist1','dist2',\n#          'P_emaildomain', 'R_emaildomain'\n#         ]\n\n#for col in i_cols:\n#    temp_df = pd.concat([train_df[[col]], test_df[[col]]])\n#    fq_encode = temp_df[col].value_counts().to_dict()   \n#    train_df[col+'_fq_enc'] = train_df[col].map(fq_encode)\n#    test_df[col+'_fq_enc']  = test_df[col].map(fq_encode)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## ProductCD and M4 Target mean\n#for col in ['ProductCD','M4']:\n#    temp_dict = train_df.groupby([col])[TARGET].agg(['mean']).reset_index().rename(\n                                                  #      columns={'mean': col+'_target_mean'})\n#    temp_dict.index = temp_dict[col].values\n#    temp_dict = temp_dict[col+'_target_mean'].to_dict()\n\n#    train_df[col+'_target_mean'] = train_df[col].map(temp_dict)\n#    test_df[col+'_target_mean']  = test_df[col].map(temp_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Encode Str columns\n#for col in list(train_df):\n#    if train_df[col].dtype=='O':\n#        print(col)\n#        train_df[col] = train_df[col].fillna('unseen_before_label')\n#        test_df[col]  = test_df[col].fillna('unseen_before_label')\n        \n#        train_df[col] = train_df[col].astype(str)\n#        test_df[col] = test_df[col].astype(str)\n        \n#        le = LabelEncoder()\n#        le.fit(list(train_df[col])+list(test_df[col]))\n#        train_df[col] = le.transform(train_df[col])\n#        test_df[col]  = le.transform(test_df[col])\n        \n#        train_df[col] = train_df[col].astype('category')\n#        test_df[col] = test_df[col].astype('category')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}